"""
Script Principal - Int√©gration Compl√®te du Pipeline
Ex√©cute le pipeline complet: Streaming + SQL + MLlib
"""

import threading
import time
import sys
from stock_data_generator import StockDataGenerator
from spark_streaming_pipeline import StockStreamingPipeline
from spark_sql_analysis import SparkSQLAnalyzer
from spark_mllib_model import StockPricePredictor

class RealTimeStockInsight:
    """
    Orchestrateur principal du pipeline Real-Time Stock Insight
    """
    
    def __init__(self):
        self.generator = None
        self.pipeline = None
        self.analyzer = None
        self.predictor = None
        self.generator_thread = None
        
    def start_data_generation(self, duration=180):
        """
        D√©marre la g√©n√©ration de donn√©es dans un thread s√©par√©
        
        Args:
            duration: Dur√©e de g√©n√©ration en secondes
        """
        print("\n" + "="*70)
        print("üìä PHASE 1: G√âN√âRATION DE DONN√âES")
        print("="*70 + "\n")
        
        self.generator = StockDataGenerator(output_path="./stream_data")
        
        # Lancer dans un thread s√©par√©
        self.generator_thread = threading.Thread(
            target=self.generator.run,
            args=(duration,)
        )
        self.generator_thread.daemon = True
        self.generator_thread.start()
        
        # Attendre que quelques fichiers soient g√©n√©r√©s
        print("‚è≥ Attente de 5 secondes pour g√©n√©rer des donn√©es initiales...\n")
        time.sleep(5)
    
    def start_streaming_pipeline(self):
        """
        D√©marre le pipeline Spark Streaming
        """
        print("\n" + "="*70)
        print("üöÄ PHASE 2: STREAMING PIPELINE")
        print("="*70 + "\n")
        
        self.pipeline = StockStreamingPipeline("RealTimeStockInsight")
        
        # Cr√©er le DataFrame en streaming
        streaming_df = self.pipeline.create_streaming_dataframe("./stream_data")
        
        # Appliquer les agr√©gations
        aggregated_df = self.pipeline.apply_windowed_aggregations(streaming_df)
        
        # √âcrire vers la m√©moire et la console
        memory_query = self.pipeline.write_to_memory(aggregated_df, "stock_aggregates")
        
        # Aussi √©crire vers Parquet pour l'entra√Ænement MLlib
        parquet_query = self.pipeline.write_to_parquet(streaming_df, "./output/stock_data")
        
        return memory_query, parquet_query
    
    def run_sql_analysis(self):
        """
        Ex√©cute l'analyse Spark SQL
        """
        print("\n" + "="*70)
        print("üìä PHASE 3: ANALYSE SPARK SQL")
        print("="*70 + "\n")
        
        # Attendre que des donn√©es s'accumulent
        print("‚è≥ Attente de 20 secondes pour accumuler des donn√©es...\n")
        time.sleep(20)
        
        spark = self.pipeline.get_spark_session()
        self.analyzer = SparkSQLAnalyzer(spark)
        self.analyzer.run_all_analyses("stock_aggregates")
    
    def run_ml_training(self):
        """
        Ex√©cute l'entra√Ænement MLlib
        """
        print("\n" + "="*70)
        print("ü§ñ PHASE 4: ENTRA√éNEMENT MLLIB")
        print("="*70 + "\n")
        
        # Attendre que suffisamment de donn√©es soient √©crites
        print("‚è≥ Attente de 15 secondes pour collecter des donn√©es d'entra√Ænement...\n")
        time.sleep(15)
        
        spark = self.pipeline.get_spark_session()
        self.predictor = StockPricePredictor(spark)
        results = self.predictor.run_full_training()
        
        return results
    
    def run_complete_pipeline(self, duration=120):
        """
        Ex√©cute le pipeline complet
        
        Args:
            duration: Dur√©e totale d'ex√©cution en secondes
        """
        print("\n" + "="*80)
        print("üéØ D√âMARRAGE DU PIPELINE COMPLET: REAL-TIME STOCK INSIGHT")
        print("="*80)
        print("\nCe pipeline d√©montre:")
        print("  1Ô∏è‚É£  Streaming en temps r√©el avec Spark Structured Streaming")
        print("  2Ô∏è‚É£  Agr√©gations par fen√™tres temporelles")
        print("  3Ô∏è‚É£  Analyse SQL avec cache et optimisation Catalyst")
        print("  4Ô∏è‚É£  Mod√©lisation pr√©dictive avec MLlib (LR + Random Forest)")
        print("\n" + "="*80 + "\n")
        
        try:
            # Phase 1: G√©n√©ration de donn√©es
            self.start_data_generation(duration=duration + 60)
            
            # Phase 2: Streaming
            memory_query, parquet_query = self.start_streaming_pipeline()
            
            # Phase 3: Analyse SQL
            self.run_sql_analysis()
            
            # Phase 4: Machine Learning
            ml_results = self.run_ml_training()
            
            # Continuer le streaming pendant un moment
            print("\n" + "="*70)
            print("‚è≥ PHASE 5: MONITORING CONTINU")
            print("="*70 + "\n")
            print("Le pipeline continue √† traiter les donn√©es...")
            print("Appuyez sur Ctrl+C pour arr√™ter.\n")
            
            # Requ√™tes p√©riodiques pendant que le streaming continue
            remaining_time = duration - 60  # On a d√©j√† attendu ~60 secondes
            if remaining_time > 0:
                memory_query.awaitTermination(timeout=remaining_time)
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur")
        except Exception as e:
            print(f"\n\n‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()
    
    def cleanup(self):
        """
        Nettoie les ressources
        """
        import time
        
        print("\n" + "="*70)
        print("üßπ NETTOYAGE")
        print("="*70 + "\n")
        
        if self.pipeline:
            try:
                self.pipeline.stop()
                print("‚úÖ Pipeline arr√™t√© proprement")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors du nettoyage (peut √™tre ignor√©e): {e}")
        
        # Attendre que tous les processus se terminent proprement
        time.sleep(1)
        
        # R√©sum√© final
        self.print_summary()
    
    def print_summary(self):
        """
        Affiche un r√©sum√© de l'ex√©cution
        """
        print("\n" + "="*70)
        print("üìä R√âSUM√â DE L'EX√âCUTION")
        print("="*70)
        print("\n‚úÖ T√ÇCHES COMPL√âT√âES:")
        print("  ‚úîÔ∏è  T√¢che 1: Ingestion temps r√©el avec Spark Streaming")
        print("      - Agr√©gations par fen√™tres temporelles (10s)")
        print("      - Calcul de volatilit√© et statistiques")
        print("\n  ‚úîÔ∏è  T√¢che 2: Analyse avec Spark SQL")
        print("      - Requ√™tes SQL sur vues temporaires")
        print("      - Comparaison avec/sans cache")
        print("      - Analyse du plan d'ex√©cution (Catalyst)")
        print("\n  ‚úîÔ∏è  T√¢che 3: Mod√©lisation MLlib")
        print("      - R√©gression Logistique")
        print("      - Random Forest")
        print("      - Pr√©diction d'augmentation de prix")
        print("\nüìÅ FICHIERS G√âN√âR√âS:")
        print("  - ./stream_data/ : Donn√©es JSON en streaming")
        print("  - ./output/stock_data/ : Donn√©es Parquet pour analyse batch")
        print("  - ./checkpoints/ : Checkpoints du streaming")
        print("\nüí° PROCHAINES √âTAPES:")
        print("  - Consultez le RAPPORT.md pour l'analyse d√©taill√©e")
        print("  - Examinez les captures d'√©cran dans ./screenshots/")
        print("  - Explorez les visualisations dans visualization.py")
        print("\n" + "="*70 + "\n")

def main():
    """
    Point d'entr√©e principal
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Pipeline Real-Time Stock Insight avec Apache Spark"
    )
    parser.add_argument(
        "--duration",
        type=int,
        default=120,
        help="Dur√©e d'ex√©cution en secondes (d√©faut: 120)"
    )
    parser.add_argument(
        "--mode",
        choices=["full", "streaming", "sql", "ml"],
        default="full",
        help="Mode d'ex√©cution (d√©faut: full)"
    )
    
    args = parser.parse_args()
    
    app = RealTimeStockInsight()
    
    if args.mode == "full":
        app.run_complete_pipeline(duration=args.duration)
    elif args.mode == "streaming":
        app.start_data_generation(duration=args.duration)
        app.start_streaming_pipeline()
    elif args.mode == "sql":
        # N√©cessite que le streaming soit d√©j√† actif
        print("‚ö†Ô∏è  Assurez-vous que le streaming est actif dans un autre terminal")
        app.run_sql_analysis()
    elif args.mode == "ml":
        app.run_ml_training()

if __name__ == "__main__":
    main()

