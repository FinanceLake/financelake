"""
Batch Job - Historisation des donn√©es
Job batch pour traiter et historiser les donn√©es depuis les tables Delta
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, year, month, dayofmonth, date_format, current_timestamp,
    avg, sum as spark_sum, count, max as spark_max, min as spark_min
)
from delta.tables import DeltaTable
from config import DELTA_BRONZE_PATH, DELTA_SILVER_PATH, DELTA_GOLD_PATH
import os

class BatchHistoricizationJob:
    """
    Job batch pour historiser les donn√©es depuis les tables Delta
    """
    
    def __init__(self, app_name="BatchHistoricization"):
        """
        Initialise la session Spark avec support Delta Lake
        """
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master("local[*]") \
            .config("spark.sql.shuffle.partitions", "4") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
            .config("spark.ui.enabled", "false") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("ERROR")
        
        # Dossier pour les donn√©es historis√©es
        self.historic_path = "./delta/historic"
        os.makedirs(self.historic_path, exist_ok=True)
        
        print("‚úÖ Job batch d'historisation initialis√©")
    
    def historize_bronze_data(self, date_partition=True):
        """
        Historise les donn√©es Bronze (raw data) avec partitionnement par date
        
        Args:
            date_partition: Si True, partitionne par date
        """
        print("\n" + "="*60)
        print("üì¶ HISTORISATION DES DONN√âES BRONZE")
        print("="*60 + "\n")
        
        if not DeltaTable.isDeltaTable(self.spark, DELTA_BRONZE_PATH):
            print(f"‚ö†Ô∏è  Table Bronze n'existe pas encore: {DELTA_BRONZE_PATH}")
            return
        
        # Lire les donn√©es Bronze
        bronze_df = self.spark.read.format("delta").load(DELTA_BRONZE_PATH)
        
        print(f"üìä Nombre d'enregistrements √† historiser: {bronze_df.count()}")
        
        if date_partition:
            # Ajouter des colonnes de partition par date
            bronze_df = bronze_df.withColumn("year", year(col("timestamp"))) \
                                 .withColumn("month", month(col("timestamp"))) \
                                 .withColumn("day", dayofmonth(col("timestamp"))) \
                                 .withColumn("date", date_format(col("timestamp"), "yyyy-MM-dd"))
        
        # Chemin de sortie
        historic_bronze_path = f"{self.historic_path}/bronze"
        
        # √âcrire avec partitionnement par date si activ√©
        write_mode = bronze_df.write.format("delta").mode("overwrite")
        
        if date_partition:
            write_mode = write_mode.partitionBy("year", "month", "day")
        
        write_mode.save(historic_bronze_path)
        
        print(f"‚úÖ Donn√©es Bronze historis√©es: {historic_bronze_path}")
        
        # Afficher un r√©sum√©
        historic_df = self.spark.read.format("delta").load(historic_bronze_path)
        print(f"üìä Enregistrements historis√©s: {historic_df.count()}")
        if date_partition:
            print("üìÖ Partitionnement par date: year/month/day")
    
    def historize_silver_data(self, date_partition=True):
        """
        Historise les donn√©es Silver (cleaned data) avec agr√©gations quotidiennes
        
        Args:
            date_partition: Si True, partitionne par date
        """
        print("\n" + "="*60)
        print("ü™ô HISTORISATION DES DONN√âES SILVER")
        print("="*60 + "\n")
        
        if not DeltaTable.isDeltaTable(self.spark, DELTA_SILVER_PATH):
            print(f"‚ö†Ô∏è  Table Silver n'existe pas encore: {DELTA_SILVER_PATH}")
            return
        
        # Lire les donn√©es Silver
        silver_df = self.spark.read.format("delta").load(DELTA_SILVER_PATH)
        
        print(f"üìä Nombre d'enregistrements √† historiser: {silver_df.count()}")
        
        # Agr√©ger par jour et symbole pour cr√©er un r√©sum√© quotidien
        daily_summary = silver_df \
            .withColumn("date", date_format(col("window_start"), "yyyy-MM-dd")) \
            .groupBy("date", "symbol") \
            .agg(
                avg("avg_price").alias("daily_avg_price"),
                avg("volatility").alias("daily_avg_volatility"),
                spark_sum("total_volume").alias("daily_total_volume"),
                spark_sum("transaction_count").alias("daily_transaction_count"),
                spark_min("min_price").alias("daily_min_price"),
                spark_max("max_price").alias("daily_max_price"),
                avg("avg_price_change").alias("daily_avg_price_change"),
                spark_max("window_start").alias("last_window_time")
            ) \
            .withColumn("year", year(col("date"))) \
            .withColumn("month", month(col("date"))) \
            .withColumn("day", dayofmonth(col("date")))
        
        # Chemin de sortie
        historic_silver_path = f"{self.historic_path}/silver"
        
        # √âcrire avec partitionnement
        write_mode = daily_summary.write.format("delta").mode("overwrite")
        
        if date_partition:
            write_mode = write_mode.partitionBy("year", "month", "day")
        
        write_mode.save(historic_silver_path)
        
        print(f"‚úÖ Donn√©es Silver historis√©es (r√©sum√© quotidien): {historic_silver_path}")
        
        # Afficher un r√©sum√©
        historic_df = self.spark.read.format("delta").load(historic_silver_path)
        print(f"üìä Enregistrements historis√©s: {historic_df.count()}")
        print("üìÖ Agr√©gation: R√©sum√© quotidien par symbole")
    
    def historize_gold_data(self):
        """
        Historise les donn√©es Gold (business metrics) avec snapshot quotidien
        """
        print("\n" + "="*60)
        print("üèÜ HISTORISATION DES DONN√âES GOLD")
        print("="*60 + "\n")
        
        if not DeltaTable.isDeltaTable(self.spark, DELTA_GOLD_PATH):
            print(f"‚ö†Ô∏è  Table Gold n'existe pas encore: {DELTA_GOLD_PATH}")
            return
        
        # Lire les donn√©es Gold
        gold_df = self.spark.read.format("delta").load(DELTA_GOLD_PATH)
        
        print(f"üìä Nombre d'enregistrements √† historiser: {gold_df.count()}")
        
        # Ajouter la date du snapshot
        gold_with_date = gold_df \
            .withColumn("snapshot_date", date_format(current_timestamp(), "yyyy-MM-dd")) \
            .withColumn("year", year(col("snapshot_date"))) \
            .withColumn("month", month(col("snapshot_date"))) \
            .withColumn("day", dayofmonth(col("snapshot_date")))
        
        # Chemin de sortie
        historic_gold_path = f"{self.historic_path}/gold"
        
        # √âcrire avec partitionnement par date
        gold_with_date.write \
            .format("delta") \
            .mode("append") \
            .partitionBy("year", "month", "day") \
            .save(historic_gold_path)
        
        print(f"‚úÖ Donn√©es Gold historis√©es (snapshot quotidien): {historic_gold_path}")
        
        # Afficher un r√©sum√©
        historic_df = self.spark.read.format("delta").load(historic_gold_path)
        print(f"üìä Enregistrements historis√©s: {historic_df.count()}")
        print("üìÖ Snapshot: √âtat quotidien des m√©triques business")
    
    def create_daily_report(self):
        """
        Cr√©e un rapport quotidien agr√©g√© depuis les donn√©es historis√©es
        """
        print("\n" + "="*60)
        print("üìä CR√âATION DU RAPPORT QUOTIDIEN")
        print("="*60 + "\n")
        
        historic_silver_path = f"{self.historic_path}/silver"
        
        if not DeltaTable.isDeltaTable(self.spark, historic_silver_path):
            print("‚ö†Ô∏è  Donn√©es historis√©es Silver non disponibles")
            return
        
        # Lire les donn√©es historis√©es
        historic_df = self.spark.read.format("delta").load(historic_silver_path)
        
        # Cr√©er un rapport quotidien
        daily_report = historic_df \
            .groupBy("date") \
            .agg(
                count("symbol").alias("symbols_tracked"),
                spark_sum("daily_total_volume").alias("total_market_volume"),
                avg("daily_avg_volatility").alias("market_avg_volatility"),
                count("*").alias("total_records")
            ) \
            .orderBy(col("date").desc())
        
        print("üìã Rapport quotidien:")
        daily_report.show(truncate=False)
        
        # Sauvegarder le rapport
        report_path = f"{self.historic_path}/daily_reports"
        daily_report.write \
            .format("delta") \
            .mode("overwrite") \
            .save(report_path)
        
        print(f"‚úÖ Rapport sauvegard√©: {report_path}")
    
    def run_full_historization(self):
        """
        Ex√©cute le job complet d'historisation
        """
        print("\n" + "="*70)
        print("üöÄ D√âMARRAGE DU JOB BATCH D'HISTORISATION")
        print("="*70 + "\n")
        
        try:
            # Historiser chaque couche
            self.historize_bronze_data()
            self.historize_silver_data()
            self.historize_gold_data()
            
            # Cr√©er le rapport quotidien
            self.create_daily_report()
            
            print("\n‚úÖ Job batch d'historisation termin√© avec succ√®s!")
            print(f"üìÅ Donn√©es historis√©es disponibles dans: {self.historic_path}/")
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de l'historisation: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.spark.stop()
    
    def get_spark_session(self):
        """
        Retourne la session Spark
        """
        return self.spark

if __name__ == "__main__":
    job = BatchHistoricizationJob()
    job.run_full_historization()

