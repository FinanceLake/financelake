"""
Silver to Gold Transformation
Transformations SQL/DataFrame pour passer de Silver Ã  Gold
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, avg, sum as spark_sum, count, max as spark_max, min as spark_min,
    when, current_timestamp, window, stddev
)
from delta.tables import DeltaTable
from config import DELTA_SILVER_PATH, DELTA_GOLD_PATH

class SilverToGoldTransformer:
    """
    Transforme les donnÃ©es Silver en donnÃ©es Gold avec mÃ©triques business
    """
    
    def __init__(self, spark_session):
        """
        Initialise le transformateur avec une session Spark
        
        Args:
            spark_session: Session Spark active
        """
        self.spark = spark_session
        print("âœ… Transformateur Silver â†’ Gold initialisÃ©")
    
    def transform_with_dataframe(self):
        """
        Transformation Silver â†’ Gold en utilisant l'API DataFrame
        """
        print("\n" + "="*60)
        print("ðŸ”„ TRANSFORMATION SILVER â†’ GOLD (DataFrame API)")
        print("="*60 + "\n")
        
        if not DeltaTable.isDeltaTable(self.spark, DELTA_SILVER_PATH):
            print(f"âš ï¸  Table Silver n'existe pas: {DELTA_SILVER_PATH}")
            return None
        
        # Lire les donnÃ©es Silver
        silver_df = self.spark.read.format("delta").load(DELTA_SILVER_PATH)
        
        print(f"ðŸ“Š Enregistrements Silver: {silver_df.count()}")
        
        # Transformation: AgrÃ©ger par symbole pour crÃ©er des mÃ©triques business
        gold_df = silver_df \
            .groupBy("symbol") \
            .agg(
                # MÃ©triques de prix
                avg("avg_price").alias("overall_avg_price"),
                spark_min("min_price").alias("absolute_min_price"),
                spark_max("max_price").alias("absolute_max_price"),
                
                # MÃ©triques de volatilitÃ©
                avg("volatility").alias("overall_volatility"),
                stddev("volatility").alias("volatility_stddev"),
                
                # MÃ©triques de volume
                spark_sum("total_volume").alias("cumulative_volume"),
                avg("total_volume").alias("avg_volume_per_window"),
                
                # MÃ©triques de transactions
                spark_sum("transaction_count").alias("total_transactions"),
                avg("transaction_count").alias("avg_transactions_per_window"),
                
                # MÃ©triques de tendance
                avg("avg_price_change").alias("trend_direction"),
                count("*").alias("window_count"),
                
                # Timestamps
                spark_max("window_start").alias("last_window_time"),
                spark_max("last_updated").alias("last_silver_update")
            ) \
            .withColumn(
                "price_trend",
                when(col("trend_direction") > 0.1, "STRONG_UP")
                .when(col("trend_direction") > 0, "UP")
                .when(col("trend_direction") < -0.1, "STRONG_DOWN")
                .when(col("trend_direction") < 0, "DOWN")
                .otherwise("STABLE")
            ) \
            .withColumn(
                "volatility_category",
                when(col("overall_volatility") > 5, "HIGH")
                .when(col("overall_volatility") > 2, "MEDIUM")
                .otherwise("LOW")
            ) \
            .withColumn(
                "volume_category",
                when(col("cumulative_volume") > 10000000, "HIGH")
                .when(col("cumulative_volume") > 5000000, "MEDIUM")
                .otherwise("LOW")
            ) \
            .withColumn(
                "price_range",
                col("absolute_max_price") - col("absolute_min_price")
            ) \
            .withColumn(
                "price_range_percent",
                (col("price_range") / col("overall_avg_price")) * 100
            ) \
            .withColumn("gold_ingestion_time", current_timestamp())
        
        print("âœ… Transformation DataFrame terminÃ©e")
        print("ðŸ“Š Colonnes crÃ©Ã©es:")
        gold_df.printSchema()
        
        return gold_df
    
    def transform_with_sql(self):
        """
        Transformation Silver â†’ Gold en utilisant Spark SQL
        """
        print("\n" + "="*60)
        print("ðŸ”„ TRANSFORMATION SILVER â†’ GOLD (Spark SQL)")
        print("="*60 + "\n")
        
        if not DeltaTable.isDeltaTable(self.spark, DELTA_SILVER_PATH):
            print(f"âš ï¸  Table Silver n'existe pas: {DELTA_SILVER_PATH}")
            return None
        
        # CrÃ©er une vue temporaire depuis Silver
        silver_df = self.spark.read.format("delta").load(DELTA_SILVER_PATH)
        silver_df.createOrReplaceTempView("silver_stock_data")
        
        # RequÃªte SQL pour transformation
        sql_query = """
        SELECT 
            symbol,
            
            -- MÃ©triques de prix
            AVG(avg_price) as overall_avg_price,
            MIN(min_price) as absolute_min_price,
            MAX(max_price) as absolute_max_price,
            MAX(max_price) - MIN(min_price) as price_range,
            ((MAX(max_price) - MIN(min_price)) / AVG(avg_price)) * 100 as price_range_percent,
            
            -- MÃ©triques de volatilitÃ©
            AVG(volatility) as overall_volatility,
            STDDEV(volatility) as volatility_stddev,
            CASE 
                WHEN AVG(volatility) > 5 THEN 'HIGH'
                WHEN AVG(volatility) > 2 THEN 'MEDIUM'
                ELSE 'LOW'
            END as volatility_category,
            
            -- MÃ©triques de volume
            SUM(total_volume) as cumulative_volume,
            AVG(total_volume) as avg_volume_per_window,
            CASE 
                WHEN SUM(total_volume) > 10000000 THEN 'HIGH'
                WHEN SUM(total_volume) > 5000000 THEN 'MEDIUM'
                ELSE 'LOW'
            END as volume_category,
            
            -- MÃ©triques de transactions
            SUM(transaction_count) as total_transactions,
            AVG(transaction_count) as avg_transactions_per_window,
            
            -- MÃ©triques de tendance
            AVG(avg_price_change) as trend_direction,
            CASE 
                WHEN AVG(avg_price_change) > 0.1 THEN 'STRONG_UP'
                WHEN AVG(avg_price_change) > 0 THEN 'UP'
                WHEN AVG(avg_price_change) < -0.1 THEN 'STRONG_DOWN'
                WHEN AVG(avg_price_change) < 0 THEN 'DOWN'
                ELSE 'STABLE'
            END as price_trend,
            
            -- Compteurs
            COUNT(*) as window_count,
            
            -- Timestamps
            MAX(window_start) as last_window_time,
            MAX(last_updated) as last_silver_update,
            CURRENT_TIMESTAMP() as gold_ingestion_time
            
        FROM silver_stock_data
        GROUP BY symbol
        ORDER BY cumulative_volume DESC
        """
        
        print("ðŸ“‹ RequÃªte SQL:")
        print(sql_query)
        print()
        
        gold_df = self.spark.sql(sql_query)
        
        print("âœ… Transformation SQL terminÃ©e")
        print("ðŸ“Š RÃ©sultats:")
        gold_df.show(truncate=False)
        
        return gold_df
    
    def write_to_gold(self, gold_df, merge=True):
        """
        Ã‰crit les donnÃ©es transformÃ©es vers la table Gold
        
        Args:
            gold_df: DataFrame Gold Ã  Ã©crire
            merge: Si True, utilise MERGE pour Ã©viter les doublons
        """
        print("\n" + "="*60)
        print("ðŸ’¾ Ã‰CRITURE VERS LA TABLE GOLD")
        print("="*60 + "\n")
        
        if gold_df is None:
            print("âŒ Aucune donnÃ©e Ã  Ã©crire")
            return
        
        if merge and DeltaTable.isDeltaTable(self.spark, DELTA_GOLD_PATH):
            # Utiliser MERGE pour mettre Ã  jour ou insÃ©rer
            print("ðŸ”„ Utilisation de MERGE pour mettre Ã  jour la table Gold...")
            
            delta_table = DeltaTable.forPath(self.spark, DELTA_GOLD_PATH)
            
            delta_table.alias("target").merge(
                gold_df.alias("source"),
                "target.symbol = source.symbol"
            ).whenMatchedUpdateAll() \
             .whenNotMatchedInsertAll() \
             .execute()
            
            print("âœ… Table Gold mise Ã  jour avec MERGE")
        else:
            # CrÃ©er ou Ã©craser la table
            print("ðŸ“ CrÃ©ation/Ã©crasement de la table Gold...")
            
            gold_df.write \
                .format("delta") \
                .mode("overwrite" if not merge else "append") \
                .save(DELTA_GOLD_PATH)
            
            print("âœ… Table Gold crÃ©Ã©e/mise Ã  jour")
        
        # Afficher le rÃ©sultat
        result_df = self.spark.read.format("delta").load(DELTA_GOLD_PATH)
        print(f"\nðŸ“Š Enregistrements dans Gold: {result_df.count()}")
        result_df.show(truncate=False)
    
    def run_transformation(self, use_sql=False, merge=True):
        """
        ExÃ©cute la transformation complÃ¨te Silver â†’ Gold
        
        Args:
            use_sql: Si True, utilise SQL au lieu de DataFrame API
            merge: Si True, utilise MERGE pour Ã©crire vers Gold
        """
        print("\n" + "="*70)
        print("ðŸš€ TRANSFORMATION COMPLÃˆTE SILVER â†’ GOLD")
        print("="*70 + "\n")
        
        try:
            # Transformer selon la mÃ©thode choisie
            if use_sql:
                gold_df = self.transform_with_sql()
            else:
                gold_df = self.transform_with_dataframe()
            
            # Ã‰crire vers Gold
            if gold_df is not None:
                self.write_to_gold(gold_df, merge=merge)
                print("\nâœ… Transformation complÃ¨te terminÃ©e!")
            else:
                print("\nâš ï¸  Aucune transformation effectuÃ©e")
                
        except Exception as e:
            print(f"\nâŒ Erreur lors de la transformation: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder \
        .appName("SilverToGoldTransformation") \
        .master("local[*]") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    transformer = SilverToGoldTransformer(spark)
    
    # Tester avec DataFrame API
    transformer.run_transformation(use_sql=False, merge=True)
    
    # Tester avec SQL
    # transformer.run_transformation(use_sql=True, merge=True)
    
    spark.stop()

