"""
Spark Structured Streaming Pipeline - T√¢che 1
Pipeline d'ingestion et traitement en temps r√©el des donn√©es boursi√®res
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, window, avg, stddev, count, 
    max as spark_max, min as spark_min, sum as spark_sum, current_timestamp
)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType
from config import WINDOW_DURATION, SLIDE_DURATION, CHECKPOINT_DIR, DELTA_BRONZE_PATH, DELTA_CHECKPOINT_DIR
import os

class StockStreamingPipeline:
    """
    Pipeline de streaming Spark pour l'analyse en temps r√©el des actions boursi√®res
    """
    
    def __init__(self, app_name="RealTimeStockInsight", enable_delta=False):
        """
        Initialise la session Spark avec les configurations optimales
        
        Args:
            app_name: Nom de l'application Spark
            enable_delta: Si True, active le support Delta Lake
        """
        builder = SparkSession.builder \
            .appName(app_name) \
            .master("local[*]") \
            .config("spark.sql.shuffle.partitions", "4") \
            .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_DIR) \
            .config("spark.ui.enabled", "false") \
            .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
            .config("spark.driver.host", "localhost")
        
        # Ajouter support Delta Lake si demand√©
        if enable_delta:
            builder = builder \
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        
        self.spark = builder.getOrCreate()
        
        # R√©duire la verbosit√© des logs
        self.spark.sparkContext.setLogLevel("ERROR")
        
        # Liste pour suivre les queries actives
        self.active_queries = []
        
        self.enable_delta = enable_delta
        print("‚úÖ Session Spark initialis√©e avec succ√®s")
        if enable_delta:
            print("‚úÖ Support Delta Lake activ√©")
    
    def define_schema(self):
        """
        D√©finit le sch√©ma explicite pour les donn√©es JSON entrantes
        
        Schema: symbol, price, volume, timestamp, price_change
        """
        schema = StructType([
            StructField("symbol", StringType(), False),
            StructField("price", DoubleType(), False),
            StructField("volume", IntegerType(), False),
            StructField("timestamp", StringType(), False),
            StructField("price_change", DoubleType(), False)
        ])
        return schema
    
    def create_streaming_dataframe(self, input_path="./stream_data"):
        """
        Cr√©e un DataFrame en streaming √† partir de fichiers JSON
        
        Args:
            input_path: Chemin vers le dossier contenant les fichiers JSON
            
        Returns:
            DataFrame en streaming avec sch√©ma pars√©
        """
        schema = self.define_schema()
        
        # Lecture du flux de fichiers JSON
        streaming_df = self.spark \
            .readStream \
            .schema(schema) \
            .json(input_path)
        
        # Conversion du timestamp string en TimestampType
        streaming_df = streaming_df.withColumn(
            "timestamp", 
            col("timestamp").cast(TimestampType())
        )
        
        print(f"üìä DataFrame en streaming cr√©√© depuis: {input_path}")
        print("üìã Sch√©ma d√©fini:")
        streaming_df.printSchema()
        
        return streaming_df
    
    def apply_windowed_aggregations(self, streaming_df):
        """
        Applique des agr√©gations glissantes par fen√™tre temporelle
        
        Calcule pour chaque symbole et fen√™tre de temps:
        - Prix moyen
        - Volatilit√© (√©cart-type des prix)
        - Volume total
        - Nombre de transactions
        - Prix min/max
        
        Args:
            streaming_df: DataFrame en streaming
            
        Returns:
            DataFrame agr√©g√© par fen√™tre temporelle
        """
        windowed_df = streaming_df \
            .groupBy(
                window(col("timestamp"), WINDOW_DURATION, SLIDE_DURATION),
                col("symbol")
            ) \
            .agg(
                avg("price").alias("avg_price"),
                stddev("price").alias("volatility"),
                spark_sum("volume").alias("total_volume"),
                count("*").alias("transaction_count"),
                spark_min("price").alias("min_price"),
                spark_max("price").alias("max_price"),
                avg("price_change").alias("avg_price_change")
            ) \
            .select(
                col("window.start").alias("window_start"),
                col("window.end").alias("window_end"),
                "symbol",
                "avg_price",
                "volatility",
                "total_volume",
                "transaction_count",
                "min_price",
                "max_price",
                "avg_price_change"
            )
        
        print(f"üîÑ Agr√©gations configur√©es:")
        print(f"   - Fen√™tre: {WINDOW_DURATION}")
        print(f"   - Glissement: {SLIDE_DURATION}")
        print(f"   - M√©triques: prix moyen, volatilit√©, volume, min/max")
        
        return windowed_df
    
    def write_to_console(self, df, query_name="console_output"):
        """
        √âcrit le flux de sortie vers la console (pour debugging)
        """
        query = df.writeStream \
            .outputMode("complete") \
            .format("console") \
            .queryName(query_name) \
            .option("truncate", "false") \
            .trigger(processingTime='5 seconds') \
            .start()
        
        self.active_queries.append(query)
        return query
    
    def write_to_memory(self, df, table_name="stock_aggregates"):
        """
        √âcrit le flux vers une table en m√©moire pour interrogation SQL
        
        Args:
            df: DataFrame √† √©crire
            table_name: Nom de la table en m√©moire
            
        Returns:
            StreamingQuery
        """
        query = df.writeStream \
            .outputMode("complete") \
            .format("memory") \
            .queryName(table_name) \
            .trigger(processingTime='5 seconds') \
            .start()
        
        print(f"üíæ Flux √©crit vers la table en m√©moire: {table_name}")
        
        self.active_queries.append(query)
        return query
    
    def write_to_parquet(self, df, output_path="./output/stock_data"):
        """
        √âcrit le flux vers des fichiers Parquet pour analyse batch ult√©rieure
        
        Args:
            df: DataFrame √† √©crire
            output_path: Chemin de sortie
            
        Returns:
            StreamingQuery
        """
        os.makedirs(output_path, exist_ok=True)
        
        query = df.writeStream \
            .outputMode("append") \
            .format("parquet") \
            .option("path", output_path) \
            .option("checkpointLocation", f"{CHECKPOINT_DIR}/parquet") \
            .trigger(processingTime='10 seconds') \
            .start()
        
        print(f"üíæ Flux √©crit vers Parquet: {output_path}")
        
        self.active_queries.append(query)
        return query
    
    def write_to_delta(self, df, output_path=None, table_name="bronze_stock_data"):
        """
        √âcrit le flux vers une table Delta Lake
        
        Args:
            df: DataFrame √† √©crire
            output_path: Chemin de sortie (d√©faut: DELTA_BRONZE_PATH)
            table_name: Nom de la table pour le checkpoint
            
        Returns:
            StreamingQuery
        """
        if not self.enable_delta:
            print("‚ö†Ô∏è  Delta Lake non activ√©. Utilisez enable_delta=True lors de l'initialisation.")
            return None
        
        if output_path is None:
            output_path = DELTA_BRONZE_PATH
        
        os.makedirs(output_path, exist_ok=True)
        os.makedirs(DELTA_CHECKPOINT_DIR, exist_ok=True)
        
        # Ajouter timestamp d'ingestion
        df_with_timestamp = df.withColumn("ingestion_timestamp", current_timestamp())
        
        query = df_with_timestamp.writeStream \
            .outputMode("append") \
            .format("delta") \
            .option("path", output_path) \
            .option("checkpointLocation", f"{DELTA_CHECKPOINT_DIR}/{table_name}") \
            .trigger(processingTime='5 seconds') \
            .start()
        
        print(f"üíæ Flux √©crit vers Delta Lake: {output_path}")
        
        self.active_queries.append(query)
        return query
    
    def run_pipeline(self, input_path="./stream_data", duration_seconds=300):
        """
        Ex√©cute le pipeline complet
        
        Args:
            input_path: Chemin des donn√©es sources
            duration_seconds: Dur√©e d'ex√©cution (None = ind√©fini)
        """
        print("\n" + "="*60)
        print("üöÄ D√âMARRAGE DU PIPELINE SPARK STREAMING")
        print("="*60 + "\n")
        
        # Cr√©er le DataFrame en streaming
        streaming_df = self.create_streaming_dataframe(input_path)
        
        # Appliquer les agr√©gations
        aggregated_df = self.apply_windowed_aggregations(streaming_df)
        
        # √âcrire vers plusieurs destinations
        console_query = self.write_to_console(aggregated_df, "console_output")
        memory_query = self.write_to_memory(aggregated_df, "stock_aggregates")
        
        # Optionnel: √©crire vers Parquet pour analyse batch
        # parquet_query = self.write_to_parquet(streaming_df, "./output/stock_data")
        
        print("\n‚úÖ Pipeline d√©marr√© avec succ√®s!")
        print("üìä Les donn√©es agr√©g√©es sont disponibles dans la table 'stock_aggregates'")
        print(f"‚è±Ô∏è  Dur√©e pr√©vue: {duration_seconds} secondes\n")
        
        # Attendre la fin du traitement
        try:
            if duration_seconds:
                console_query.awaitTermination(timeout=duration_seconds)
            else:
                console_query.awaitTermination()
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Interruption du pipeline...")
        finally:
            self.stop()
    
    def stop(self):
        """
        Arr√™te proprement la session Spark et toutes les queries actives
        """
        import time
        
        # Arr√™ter toutes les queries en streaming
        print("üõë Arr√™t des queries en streaming...")
        for query in self.active_queries:
            try:
                if query and query.isActive:
                    query.stop()
                    print(f"  ‚úì Query '{query.name}' arr√™t√©e")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Erreur lors de l'arr√™t de la query: {e}")
        
        # Attendre un peu pour que les ressources se lib√®rent
        time.sleep(2)
        
        # Arr√™ter la session Spark
        if self.spark:
            try:
                self.spark.stop()
                print("üõë Session Spark arr√™t√©e proprement")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de l'arr√™t de Spark (peut √™tre ignor√©e): {e}")
    
    def get_spark_session(self):
        """
        Retourne la session Spark pour utilisation externe
        """
        return self.spark

if __name__ == "__main__":
    pipeline = StockStreamingPipeline()
    pipeline.run_pipeline(duration_seconds=120)  # Ex√©cuter pendant 2 minutes

