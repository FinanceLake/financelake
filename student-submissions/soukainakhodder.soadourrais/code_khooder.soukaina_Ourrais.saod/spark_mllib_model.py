"""
Spark MLlib Model - T√¢che 3
Mod√©lisation pr√©dictive pour pr√©dire si le prix d'une action va augmenter
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lag, when, abs as spark_abs
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline
import os

class StockPricePredictor:
    """
    Mod√®le de pr√©diction utilisant Spark MLlib
    Pr√©dit si le prix d'une action va augmenter ou diminuer
    """
    
    def __init__(self, spark_session):
        """
        Initialise le pr√©dicteur avec une session Spark
        
        Args:
            spark_session: Session Spark active
        """
        self.spark = spark_session
        self.model_lr = None
        self.model_rf = None
        print("‚úÖ Pr√©dicteur MLlib initialis√©")
    
    def prepare_training_data(self, input_path="./output/stock_data"):
        """
        Pr√©pare les donn√©es d'entra√Ænement √† partir des fichiers Parquet
        
        Cr√©e des features:
        - price_lag1, price_lag2: Prix aux temps t-1, t-2
        - volume_lag1: Volume au temps t-1
        - price_change_lag1: Changement de prix au temps t-1
        - hour, day_of_week: Features temporelles
        
        Variable cible:
        - price_increase: 1 si le prix augmente, 0 sinon
        
        Args:
            input_path: Chemin vers les donn√©es Parquet
            
        Returns:
            DataFrame pr√©par√© pour l'entra√Ænement
        """
        print("\n" + "="*60)
        print("üìä PR√âPARATION DES DONN√âES D'ENTRA√éNEMENT")
        print("="*60 + "\n")
        
        # V√©rifier si les donn√©es existent
        if not os.path.exists(input_path):
            print(f"‚ö†Ô∏è  Dossier {input_path} introuvable.")
            print("   Cr√©ation de donn√©es synth√©tiques pour la d√©monstration...")
            return self._create_synthetic_data()
        
        # Lire les donn√©es Parquet
        try:
            df = self.spark.read.parquet(input_path)
            print(f"‚úÖ Donn√©es charg√©es depuis {input_path}")
        except:
            print("‚ö†Ô∏è  Erreur de lecture des donn√©es Parquet.")
            print("   Cr√©ation de donn√©es synth√©tiques...")
            return self._create_synthetic_data()
        
        # D√©finir la fen√™tre de partition par symbole, ordonn√©e par timestamp
        window_spec = Window.partitionBy("symbol").orderBy("timestamp")
        
        # Cr√©er les features avec d√©calage temporel
        df = df.withColumn("price_lag1", lag("price", 1).over(window_spec))
        df = df.withColumn("price_lag2", lag("price", 2).over(window_spec))
        df = df.withColumn("volume_lag1", lag("volume", 1).over(window_spec))
        df = df.withColumn("price_change_lag1", lag("price_change", 1).over(window_spec))
        
        # Features temporelles
        from pyspark.sql.functions import hour, dayofweek
        df = df.withColumn("hour", hour("timestamp"))
        df = df.withColumn("day_of_week", dayofweek("timestamp"))
        
        # Variable cible: 1 si le prix augmente, 0 sinon
        df = df.withColumn(
            "price_increase",
            when(col("price_change") > 0, 1.0).otherwise(0.0)
        )
        
        # Supprimer les lignes avec valeurs nulles
        df = df.na.drop()
        
        print("üìã Features cr√©√©es:")
        print("   - price_lag1, price_lag2: Prix historiques")
        print("   - volume_lag1: Volume historique")
        print("   - price_change_lag1: Changement de prix historique")
        print("   - hour, day_of_week: Features temporelles")
        print("\nüéØ Variable cible: price_increase (1=augmentation, 0=diminution)")
        
        print(f"\nüìä Nombre d'enregistrements: {df.count()}")
        df.select("symbol", "price", "price_lag1", "price_increase").show(10)
        
        return df
    
    def _create_synthetic_data(self):
        """
        Cr√©e des donn√©es synth√©tiques pour la d√©monstration
        """
        from pyspark.sql.functions import rand, randn, when
        from config import STOCK_SYMBOLS, BASE_PRICES
        
        # Cr√©er des donn√©es synth√©tiques
        data = []
        for symbol in STOCK_SYMBOLS:
            base_price = BASE_PRICES[symbol]
            for i in range(1000):
                price = base_price * (1 + (i % 20 - 10) * 0.01)
                data.append((
                    symbol,
                    price,
                    int(100000 + (i % 500000)),
                    (i % 24),  # hour
                    (i % 7),   # day_of_week
                ))
        
        df = self.spark.createDataFrame(
            data,
            ["symbol", "price", "volume", "hour", "day_of_week"]
        )
        
        # Cr√©er les features lag
        window_spec = Window.partitionBy("symbol").orderBy("price")
        df = df.withColumn("price_lag1", lag("price", 1).over(window_spec))
        df = df.withColumn("price_lag2", lag("price", 2).over(window_spec))
        df = df.withColumn("volume_lag1", lag("volume", 1).over(window_spec))
        
        # Calculer price_change_lag1
        df = df.withColumn(
            "price_change_lag1",
            ((col("price") - col("price_lag1")) / col("price_lag1") * 100)
        )
        
        # Variable cible
        df = df.withColumn(
            "price_increase",
            when(rand() > 0.5, 1.0).otherwise(0.0)
        )
        
        df = df.na.drop()
        
        print("‚úÖ Donn√©es synth√©tiques cr√©√©es pour la d√©monstration")
        return df
    
    def build_features(self, df):
        """
        Construit le vecteur de features pour MLlib
        
        Args:
            df: DataFrame avec les features
            
        Returns:
            DataFrame avec colonne 'features'
        """
        feature_columns = [
            "price_lag1", "price_lag2", "volume_lag1",
            "price_change_lag1", "hour", "day_of_week"
        ]
        
        # Assembler les features en un vecteur
        assembler = VectorAssembler(
            inputCols=feature_columns,
            outputCol="features_raw"
        )
        
        # Normaliser les features
        scaler = StandardScaler(
            inputCol="features_raw",
            outputCol="features",
            withStd=True,
            withMean=True
        )
        
        # Appliquer la transformation
        df = assembler.transform(df)
        scaler_model = scaler.fit(df)
        df = scaler_model.transform(df)
        
        print("‚úÖ Features assembl√©es et normalis√©es")
        
        return df, feature_columns
    
    def train_logistic_regression(self, train_df, test_df):
        """
        Entra√Æne un mod√®le de r√©gression logistique
        
        Args:
            train_df: Donn√©es d'entra√Ænement
            test_df: Donn√©es de test
            
        Returns:
            Mod√®le entra√Æn√© et m√©triques
        """
        print("\n" + "="*60)
        print("ü§ñ ENTRA√éNEMENT: R√©gression Logistique")
        print("="*60 + "\n")
        
        # Cr√©er le mod√®le
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="price_increase",
            maxIter=100,
            regParam=0.01
        )
        
        # Entra√Æner
        self.model_lr = lr.fit(train_df)
        
        # Pr√©dictions
        predictions = self.model_lr.transform(test_df)
        
        # √âvaluation
        metrics = self._evaluate_model(predictions, "R√©gression Logistique")
        
        # Coefficients
        print("\nüìä Coefficients du mod√®le:")
        print(f"   Intercept: {self.model_lr.intercept:.4f}")
        print(f"   Coefficients: {self.model_lr.coefficients.toArray()}")
        
        return self.model_lr, metrics
    
    def train_random_forest(self, train_df, test_df):
        """
        Entra√Æne un mod√®le Random Forest
        
        Args:
            train_df: Donn√©es d'entra√Ænement
            test_df: Donn√©es de test
            
        Returns:
            Mod√®le entra√Æn√© et m√©triques
        """
        print("\n" + "="*60)
        print("üå≤ ENTRA√éNEMENT: Random Forest")
        print("="*60 + "\n")
        
        # Cr√©er le mod√®le
        rf = RandomForestClassifier(
            featuresCol="features",
            labelCol="price_increase",
            numTrees=100,
            maxDepth=10,
            seed=42
        )
        
        # Entra√Æner
        self.model_rf = rf.fit(train_df)
        
        # Pr√©dictions
        predictions = self.model_rf.transform(test_df)
        
        # √âvaluation
        metrics = self._evaluate_model(predictions, "Random Forest")
        
        # Importance des features
        print("\nüìä Importance des features:")
        feature_importance = self.model_rf.featureImportances.toArray()
        for i, importance in enumerate(feature_importance):
            print(f"   Feature {i}: {importance:.4f}")
        
        return self.model_rf, metrics
    
    def _evaluate_model(self, predictions, model_name):
        """
        √âvalue les performances d'un mod√®le
        
        Args:
            predictions: DataFrame avec les pr√©dictions
            model_name: Nom du mod√®le
            
        Returns:
            Dictionnaire avec les m√©triques
        """
        print(f"\nüìä √âVALUATION: {model_name}")
        print("-" * 60)
        
        # Afficher quelques pr√©dictions
        predictions.select(
            "price_increase", "prediction", "probability"
        ).show(10, truncate=False)
        
        # √âvaluateur pour classification binaire
        evaluator_auc = BinaryClassificationEvaluator(
            labelCol="price_increase",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        # √âvaluateur pour m√©triques multiclasse
        evaluator_accuracy = MulticlassClassificationEvaluator(
            labelCol="price_increase",
            predictionCol="prediction",
            metricName="accuracy"
        )
        
        evaluator_precision = MulticlassClassificationEvaluator(
            labelCol="price_increase",
            predictionCol="prediction",
            metricName="weightedPrecision"
        )
        
        evaluator_recall = MulticlassClassificationEvaluator(
            labelCol="price_increase",
            predictionCol="prediction",
            metricName="weightedRecall"
        )
        
        evaluator_f1 = MulticlassClassificationEvaluator(
            labelCol="price_increase",
            predictionCol="prediction",
            metricName="f1"
        )
        
        # Calculer les m√©triques
        auc = evaluator_auc.evaluate(predictions)
        accuracy = evaluator_accuracy.evaluate(predictions)
        precision = evaluator_precision.evaluate(predictions)
        recall = evaluator_recall.evaluate(predictions)
        f1 = evaluator_f1.evaluate(predictions)
        
        print(f"\n‚úÖ M√©triques pour {model_name}:")
        print(f"   - AUC (Area Under ROC): {auc:.4f}")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - Precision: {precision:.4f}")
        print(f"   - Recall: {recall:.4f}")
        print(f"   - F1-Score: {f1:.4f}")
        
        return {
            "auc": auc,
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def compare_models(self, metrics_lr, metrics_rf):
        """
        Compare les performances des deux mod√®les
        
        Args:
            metrics_lr: M√©triques de la r√©gression logistique
            metrics_rf: M√©triques du Random Forest
        """
        print("\n" + "="*60)
        print("‚öñÔ∏è  COMPARAISON DES MOD√àLES")
        print("="*60 + "\n")
        
        print(f"{'M√©trique':<20} {'Logistic Regression':<25} {'Random Forest':<25}")
        print("-" * 70)
        
        for metric in ["auc", "accuracy", "precision", "recall", "f1"]:
            lr_val = metrics_lr[metric]
            rf_val = metrics_rf[metric]
            winner = "üèÜ LR" if lr_val > rf_val else "üèÜ RF"
            print(f"{metric.upper():<20} {lr_val:<25.4f} {rf_val:<25.4f} {winner}")
        
        # D√©terminer le meilleur mod√®le global
        lr_avg = sum(metrics_lr.values()) / len(metrics_lr)
        rf_avg = sum(metrics_rf.values()) / len(metrics_rf)
        
        print("\n" + "="*60)
        if rf_avg > lr_avg:
            print("üèÜ GAGNANT: Random Forest")
            print(f"   Score moyen: {rf_avg:.4f} vs {lr_avg:.4f}")
        else:
            print("üèÜ GAGNANT: Logistic Regression")
            print(f"   Score moyen: {lr_avg:.4f} vs {rf_avg:.4f}")
        print("="*60)
    
    def run_full_training(self):
        """
        Ex√©cute le pipeline complet d'entra√Ænement
        """
        print("\n" + "="*70)
        print("üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT MLLIB")
        print("="*70)
        
        # Pr√©parer les donn√©es
        df = self.prepare_training_data()
        
        # Construire les features
        df, feature_columns = self.build_features(df)
        
        # Split train/test
        train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
        
        print(f"\nüìä Split des donn√©es:")
        print(f"   - Entra√Ænement: {train_df.count()} enregistrements")
        print(f"   - Test: {test_df.count()} enregistrements")
        
        # Entra√Æner les mod√®les
        model_lr, metrics_lr = self.train_logistic_regression(train_df, test_df)
        model_rf, metrics_rf = self.train_random_forest(train_df, test_df)
        
        # Comparer les mod√®les
        self.compare_models(metrics_lr, metrics_rf)
        
        print("\n‚úÖ Entra√Ænement MLlib termin√©!")
        
        return {
            "lr_model": model_lr,
            "rf_model": model_rf,
            "lr_metrics": metrics_lr,
            "rf_metrics": metrics_rf
        }

if __name__ == "__main__":
    # Ce module peut √™tre ex√©cut√© ind√©pendamment pour tester
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder \
        .appName("MLlibTest") \
        .master("local[*]") \
        .getOrCreate()
    
    predictor = StockPricePredictor(spark)
    results = predictor.run_full_training()
    
    spark.stop()

