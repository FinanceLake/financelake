"""
Main Pipeline - Orchestration compl√®te du pipeline Delta + Streaming + ML + Dashboard
Script principal pour lancer tout le pipeline de A √† Z
"""

import threading
import time
import sys
import os
from stock_data_generator import StockDataGenerator
from delta_lake_pipeline import DeltaLakePipeline
from batch_job import BatchHistoricizationJob
from silver_to_gold_transformation import SilverToGoldTransformer
from streaming_ml_scoring import StreamingMLScoring
from dashboard_generator import DashboardGenerator
from pyspark.sql import SparkSession

class CompletePipeline:
    """
    Orchestrateur complet du pipeline Real-Time Stock Insight avec Delta Lake
    """
    
    def __init__(self):
        self.generator = None
        self.delta_pipeline = None
        self.transformer = None
        self.ml_scorer = None
        self.dashboard_gen = None
        self.generator_thread = None
        self.active_queries = []
        
    def start_data_generation(self, duration=300):
        """
        D√©marre la g√©n√©ration de donn√©es dans un thread s√©par√©
        
        Args:
            duration: Dur√©e de g√©n√©ration en secondes
        """
        print("\n" + "="*70)
        print("üìä PHASE 1: G√âN√âRATION DE DONN√âES")
        print("="*70 + "\n")
        
        self.generator = StockDataGenerator(output_path="./stream_data")
        
        self.generator_thread = threading.Thread(
            target=self.generator.run,
            args=(duration,)
        )
        self.generator_thread.daemon = True
        self.generator_thread.start()
        
        print("‚è≥ Attente de 5 secondes pour g√©n√©rer des donn√©es initiales...\n")
        time.sleep(5)
    
    def start_delta_pipeline(self):
        """
        D√©marre le pipeline Delta Lake (Bronze/Silver/Gold)
        """
        print("\n" + "="*70)
        print("üöÄ PHASE 2: PIPELINE DELTA LAKE")
        print("="*70 + "\n")
        
        self.delta_pipeline = DeltaLakePipeline("DeltaLakeStockInsight")
        
        # Cr√©er les tables en streaming
        bronze_query = self.delta_pipeline.create_bronze_table("./stream_data")
        silver_query = self.delta_pipeline.create_silver_table()
        gold_query = self.delta_pipeline.create_gold_table()
        
        self.active_queries.extend([bronze_query, silver_query, gold_query])
        
        print("\n‚úÖ Pipeline Delta Lake d√©marr√©!")
        print("üì¶ Bronze: Donn√©es brutes")
        print("ü™ô Silver: Donn√©es nettoy√©es et enrichies")
        print("üèÜ Gold: M√©triques business")
        
        return bronze_query, silver_query, gold_query
    
    def run_silver_to_gold_transformation(self):
        """
        Ex√©cute la transformation Silver ‚Üí Gold
        """
        print("\n" + "="*70)
        print("üîÑ PHASE 3: TRANSFORMATION SILVER ‚Üí GOLD")
        print("="*70 + "\n")
        
        # Attendre que des donn√©es Silver soient disponibles
        print("‚è≥ Attente de 30 secondes pour accumuler des donn√©es Silver...\n")
        time.sleep(30)
        
        spark = self.delta_pipeline.get_spark_session()
        self.transformer = SilverToGoldTransformer(spark)
        
        # Ex√©cuter la transformation avec DataFrame API
        self.transformer.run_transformation(use_sql=False, merge=True)
    
    def run_ml_training_and_scoring(self):
        """
        Entra√Æne le mod√®le ML et d√©marre le scoring en streaming
        """
        print("\n" + "="*70)
        print("ü§ñ PHASE 4: MACHINE LEARNING & SCORING")
        print("="*70 + "\n")
        
        # Attendre que des donn√©es soient disponibles
        print("‚è≥ Attente de 20 secondes pour accumuler des donn√©es...\n")
        time.sleep(20)
        
        spark = self.delta_pipeline.get_spark_session()
        self.ml_scorer = StreamingMLScoring(spark)
        
        # Entra√Æner le mod√®le
        print("üîÑ Entra√Ænement du mod√®le ML...")
        self.ml_scorer.train_model_batch(use_silver=True)
        
        # D√©marrer le scoring en streaming
        print("\nüöÄ D√©marrage du scoring ML en streaming...")
        scoring_query = self.ml_scorer.create_streaming_scoring_query(use_silver=True)
        
        if scoring_query:
            self.active_queries.append(scoring_query)
            print("‚úÖ Scoring ML en streaming d√©marr√©")
    
    def run_batch_historization(self):
        """
        Ex√©cute le job batch d'historisation
        """
        print("\n" + "="*70)
        print("üì¶ PHASE 5: HISTORISATION BATCH")
        print("="*70 + "\n")
        
        # Attendre que des donn√©es soient disponibles
        print("‚è≥ Attente de 15 secondes avant l'historisation...\n")
        time.sleep(15)
        
        spark = self.delta_pipeline.get_spark_session()
        batch_job = BatchHistoricizationJob("BatchHistoricization")
        batch_job.spark = spark  # R√©utiliser la session Spark
        
        # Ex√©cuter l'historisation
        try:
            batch_job.historize_bronze_data()
            batch_job.historize_silver_data()
            batch_job.historize_gold_data()
            batch_job.create_daily_report()
            print("\n‚úÖ Historisation batch termin√©e")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de l'historisation: {e}")
    
    def generate_dashboard(self):
        """
        G√©n√®re le dashboard avec tous les graphiques
        """
        print("\n" + "="*70)
        print("üìä PHASE 6: G√âN√âRATION DU DASHBOARD")
        print("="*70 + "\n")
        
        # Attendre que des donn√©es soient disponibles
        print("‚è≥ Attente de 10 secondes avant la g√©n√©ration du dashboard...\n")
        time.sleep(10)
        
        spark = self.delta_pipeline.get_spark_session()
        self.dashboard_gen = DashboardGenerator(spark)
        
        # G√©n√©rer tous les graphiques
        self.dashboard_gen.generate_all_dashboards()
    
    def optimize_and_maintain_delta_tables(self):
        """
        Optimise et nettoie les tables Delta
        """
        print("\n" + "="*70)
        print("üîß PHASE 7: MAINTENANCE DES TABLES DELTA")
        print("="*70 + "\n")
        
        # Optimiser les tables
        self.delta_pipeline.optimize_tables()
        
        # VACUUM (optionnel, peut √™tre fait p√©riodiquement)
        # self.delta_pipeline.vacuum_tables(retention_hours=168)
    
    def run_complete_pipeline(self, duration=300):
        """
        Ex√©cute le pipeline complet
        
        Args:
            duration: Dur√©e totale d'ex√©cution en secondes
        """
        print("\n" + "="*80)
        print("üéØ D√âMARRAGE DU PIPELINE COMPLET: REAL-TIME STOCK INSIGHT")
        print("="*80)
        print("\nCe pipeline d√©montre:")
        print("  1Ô∏è‚É£  G√©n√©ration de donn√©es boursi√®res simul√©es")
        print("  2Ô∏è‚É£  Pipeline Delta Lake (Bronze/Silver/Gold)")
        print("  3Ô∏è‚É£  Streaming en temps r√©el avec Spark Structured Streaming")
        print("  4Ô∏è‚É£  Transformations SQL/DataFrame (Silver ‚Üí Gold)")
        print("  5Ô∏è‚É£  Machine Learning avec MLlib (entra√Ænement + scoring en streaming)")
        print("  6Ô∏è‚É£  Job batch d'historisation")
        print("  7Ô∏è‚É£  G√©n√©ration automatique de dashboard avec graphiques")
        print("  8Ô∏è‚É£  Op√©rations Delta (MERGE, VACUUM, OPTIMIZE, time travel)")
        print("\n" + "="*80 + "\n")
        
        try:
            # Phase 1: G√©n√©ration de donn√©es
            self.start_data_generation(duration=duration + 60)
            
            # Phase 2: Pipeline Delta Lake
            bronze_query, silver_query, gold_query = self.start_delta_pipeline()
            
            # Phase 3: Transformation Silver ‚Üí Gold
            self.run_silver_to_gold_transformation()
            
            # Phase 4: ML Training & Scoring
            self.run_ml_training_and_scoring()
            
            # Phase 5: Batch Historization (en arri√®re-plan)
            historization_thread = threading.Thread(target=self.run_batch_historization)
            historization_thread.daemon = True
            historization_thread.start()
            
            # Phase 6: Dashboard (p√©riodique)
            dashboard_thread = threading.Thread(target=self._periodic_dashboard_generation)
            dashboard_thread.daemon = True
            dashboard_thread.start()
            
            # Phase 7: Maintenance (p√©riodique)
            maintenance_thread = threading.Thread(target=self._periodic_maintenance)
            maintenance_thread.daemon = True
            maintenance_thread.start()
            
            # Continuer le streaming pendant la dur√©e sp√©cifi√©e
            print("\n" + "="*70)
            print("‚è≥ PIPELINE EN COURS D'EX√âCUTION")
            print("="*70 + "\n")
            print("Le pipeline traite les donn√©es en continu...")
            print("Appuyez sur Ctrl+C pour arr√™ter.\n")
            
            remaining_time = duration - 100  # On a d√©j√† attendu ~100 secondes
            if remaining_time > 0:
                bronze_query.awaitTermination(timeout=remaining_time)
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur")
        except Exception as e:
            print(f"\n\n‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()
    
    def _periodic_dashboard_generation(self):
        """
        G√©n√®re le dashboard p√©riodiquement
        """
        while True:
            try:
                time.sleep(60)  # G√©n√©rer toutes les 60 secondes
                if self.delta_pipeline:
                    self.generate_dashboard()
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de la g√©n√©ration du dashboard: {e}")
    
    def _periodic_maintenance(self):
        """
        Effectue la maintenance p√©riodique des tables Delta
        """
        while True:
            try:
                time.sleep(300)  # Maintenance toutes les 5 minutes
                if self.delta_pipeline:
                    self.optimize_and_maintain_delta_tables()
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de la maintenance: {e}")
    
    def cleanup(self):
        """
        Nettoie les ressources
        """
        print("\n" + "="*70)
        print("üßπ NETTOYAGE")
        print("="*70 + "\n")
        
        # Arr√™ter les queries
        if self.delta_pipeline:
            try:
                self.delta_pipeline.stop()
                print("‚úÖ Pipeline Delta Lake arr√™t√©")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors du nettoyage: {e}")
        
        time.sleep(2)
        
        # R√©sum√© final
        self.print_summary()
    
    def print_summary(self):
        """
        Affiche un r√©sum√© de l'ex√©cution
        """
        print("\n" + "="*70)
        print("üìä R√âSUM√â DE L'EX√âCUTION")
        print("="*70)
        print("\n‚úÖ T√ÇCHES COMPL√âT√âES:")
        print("  ‚úîÔ∏è  G√©n√©ration de donn√©es boursi√®res simul√©es")
        print("  ‚úîÔ∏è  Pipeline Delta Lake (Bronze/Silver/Gold)")
        print("  ‚úîÔ∏è  Streaming en temps r√©el avec Spark Structured Streaming")
        print("  ‚úîÔ∏è  Transformations SQL/DataFrame (Silver ‚Üí Gold)")
        print("  ‚úîÔ∏è  Machine Learning avec MLlib (entra√Ænement + scoring)")
        print("  ‚úîÔ∏è  Job batch d'historisation")
        print("  ‚úîÔ∏è  G√©n√©ration automatique de dashboard")
        print("  ‚úîÔ∏è  Op√©rations Delta (MERGE, VACUUM, OPTIMIZE)")
        print("\nüìÅ FICHIERS G√âN√âR√âS:")
        print("  - ./stream_data/ : Donn√©es JSON en streaming")
        print("  - ./delta/bronze/ : Table Delta Bronze (raw)")
        print("  - ./delta/silver/ : Table Delta Silver (cleaned)")
        print("  - ./delta/gold/ : Table Delta Gold (business metrics)")
        print("  - ./delta/historic/ : Donn√©es historis√©es")
        print("  - ./dashboard/screenshots/ : Graphiques du dashboard")
        print("  - ./models/ : Mod√®les ML entra√Æn√©s")
        print("  - ./checkpoints/ : Checkpoints du streaming")
        print("\nüí° PROCHAINES √âTAPES:")
        print("  - Consultez les graphiques dans ./dashboard/screenshots/")
        print("  - Examinez les interpr√©tations dans les fichiers *_interpretation.txt")
        print("  - Explorez les tables Delta avec time travel")
        print("  - Consultez la documentation mise √† jour")
        print("\n" + "="*70 + "\n")

def main():
    """
    Point d'entr√©e principal
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Pipeline complet Real-Time Stock Insight avec Delta Lake"
    )
    parser.add_argument(
        "--duration",
        type=int,
        default=300,
        help="Dur√©e d'ex√©cution en secondes (d√©faut: 300 = 5 minutes)"
    )
    parser.add_argument(
        "--mode",
        choices=["full", "delta", "ml", "dashboard", "batch"],
        default="full",
        help="Mode d'ex√©cution (d√©faut: full)"
    )
    
    args = parser.parse_args()
    
    app = CompletePipeline()
    
    if args.mode == "full":
        app.run_complete_pipeline(duration=args.duration)
    elif args.mode == "delta":
        app.start_data_generation(duration=args.duration)
        app.start_delta_pipeline()
    elif args.mode == "ml":
        app.start_data_generation(duration=args.duration)
        app.start_delta_pipeline()
        app.run_ml_training_and_scoring()
    elif args.mode == "dashboard":
        app.generate_dashboard()
    elif args.mode == "batch":
        app.run_batch_historization()

if __name__ == "__main__":
    main()

