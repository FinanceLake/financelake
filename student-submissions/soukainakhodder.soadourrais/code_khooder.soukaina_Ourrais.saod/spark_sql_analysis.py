"""
Spark SQL Analysis - T√¢che 2
Analyse des donn√©es avec Spark SQL, cache, et analyse du plan d'ex√©cution
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, stddev, count, desc
import time

class SparkSQLAnalyzer:
    """
    Analyseur utilisant Spark SQL pour interroger les donn√©es agr√©g√©es
    """
    
    def __init__(self, spark_session):
        """
        Initialise l'analyseur avec une session Spark existante
        
        Args:
            spark_session: Session Spark active
        """
        self.spark = spark_session
        print("‚úÖ Analyseur Spark SQL initialis√©")
    
    def create_temp_view(self, table_name="stock_aggregates", view_name="stock_view"):
        """
        Cr√©e une vue temporaire √† partir de la table en m√©moire
        
        Args:
            table_name: Nom de la table source
            view_name: Nom de la vue temporaire √† cr√©er
        """
        try:
            df = self.spark.table(table_name)
            df.createOrReplaceTempView(view_name)
            print(f"üìä Vue temporaire '{view_name}' cr√©√©e depuis '{table_name}'")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation de la vue: {e}")
            return False
    
    def query_average_price_by_symbol(self, view_name="stock_view"):
        """
        Requ√™te SQL: Prix moyen et volatilit√© moyenne par symbole
        
        Returns:
            DataFrame avec r√©sultats
        """
        query = f"""
        SELECT 
            symbol,
            AVG(avg_price) as overall_avg_price,
            AVG(volatility) as overall_volatility,
            SUM(total_volume) as cumulative_volume,
            COUNT(*) as window_count,
            MIN(min_price) as absolute_min_price,
            MAX(max_price) as absolute_max_price
        FROM {view_name}
        GROUP BY symbol
        ORDER BY overall_avg_price DESC
        """
        
        print("\n" + "="*60)
        print("üìä REQU√äTE SQL: Statistiques par Symbole")
        print("="*60)
        print(query)
        
        result_df = self.spark.sql(query)
        result_df.show(truncate=False)
        
        return result_df
    
    def query_top_volatile_stocks(self, view_name="stock_view", top_n=5):
        """
        Requ√™te SQL: Trouver les actions les plus volatiles
        
        Args:
            view_name: Nom de la vue temporaire
            top_n: Nombre de r√©sultats √† retourner
            
        Returns:
            DataFrame avec les actions les plus volatiles
        """
        query = f"""
        SELECT 
            symbol,
            AVG(volatility) as avg_volatility,
            MAX(volatility) as max_volatility,
            AVG(avg_price_change) as avg_price_change_pct
        FROM {view_name}
        WHERE volatility IS NOT NULL
        GROUP BY symbol
        ORDER BY avg_volatility DESC
        LIMIT {top_n}
        """
        
        print("\n" + "="*60)
        print(f"üìà REQU√äTE SQL: Top {top_n} Actions les Plus Volatiles")
        print("="*60)
        print(query)
        
        result_df = self.spark.sql(query)
        result_df.show(truncate=False)
        
        return result_df
    
    def query_high_volume_periods(self, view_name="stock_view", volume_threshold=1000000):
        """
        Requ√™te SQL: P√©riodes de volume √©lev√©
        
        Args:
            view_name: Nom de la vue temporaire
            volume_threshold: Seuil de volume
            
        Returns:
            DataFrame avec les p√©riodes de volume √©lev√©
        """
        query = f"""
        SELECT 
            window_start,
            window_end,
            symbol,
            total_volume,
            avg_price,
            transaction_count
        FROM {view_name}
        WHERE total_volume > {volume_threshold}
        ORDER BY total_volume DESC
        """
        
        print("\n" + "="*60)
        print(f"üìä REQU√äTE SQL: P√©riodes de Volume √âlev√© (> {volume_threshold:,})")
        print("="*60)
        print(query)
        
        result_df = self.spark.sql(query)
        result_df.show(truncate=False)
        
        return result_df
    
    def compare_cache_performance(self, view_name="stock_view"):
        """
        Compare les performances avec et sans cache
        
        Cette fonction d√©montre l'impact du cache sur les performances
        en ex√©cutant la m√™me requ√™te deux fois: une fois sans cache,
        et une fois avec cache.
        """
        print("\n" + "="*60)
        print("üî¨ ANALYSE DE PERFORMANCE: Cache vs No Cache")
        print("="*60 + "\n")
        
        # Requ√™te complexe pour tester
        complex_query = f"""
        SELECT 
            symbol,
            AVG(avg_price) as avg_price,
            STDDEV(avg_price) as price_std,
            AVG(volatility) as avg_volatility,
            SUM(total_volume) as total_volume
        FROM {view_name}
        GROUP BY symbol
        ORDER BY avg_volatility DESC
        """
        
        # === Test SANS cache ===
        print("1Ô∏è‚É£  Ex√©cution SANS cache...")
        df_no_cache = self.spark.sql(complex_query)
        
        start_time = time.time()
        df_no_cache.show()
        no_cache_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  Temps sans cache: {no_cache_time:.4f} secondes\n")
        
        # === Test AVEC cache ===
        print("2Ô∏è‚É£  Ex√©cution AVEC cache...")
        df_with_cache = self.spark.sql(complex_query)
        df_with_cache.cache()  # Mise en cache
        
        # Premi√®re ex√©cution (remplit le cache)
        start_time = time.time()
        df_with_cache.show()
        first_cache_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  Temps avec cache (1√®re ex√©cution): {first_cache_time:.4f} secondes")
        
        # Deuxi√®me ex√©cution (utilise le cache)
        start_time = time.time()
        df_with_cache.show()
        second_cache_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  Temps avec cache (2√®me ex√©cution): {second_cache_time:.4f} secondes\n")
        
        # R√©sum√©
        print("üìä R√âSUM√â:")
        print(f"   - Sans cache: {no_cache_time:.4f}s")
        print(f"   - Avec cache (1√®re): {first_cache_time:.4f}s")
        print(f"   - Avec cache (2√®me): {second_cache_time:.4f}s")
        
        if second_cache_time < no_cache_time:
            speedup = no_cache_time / second_cache_time
            print(f"   - Acc√©l√©ration: {speedup:.2f}x plus rapide avec cache! üöÄ")
        
        # Nettoyer le cache
        df_with_cache.unpersist()
        
        return {
            "no_cache": no_cache_time,
            "first_cache": first_cache_time,
            "second_cache": second_cache_time
        }
    
    def analyze_execution_plan(self, view_name="stock_view"):
        """
        Analyse le plan d'ex√©cution avec explain('formatted')
        
        Permet d'observer l'impact du Catalyst Optimizer
        """
        print("\n" + "="*60)
        print("üîç ANALYSE DU PLAN D'EX√âCUTION (Catalyst Optimizer)")
        print("="*60 + "\n")
        
        query = f"""
        SELECT 
            symbol,
            AVG(avg_price) as avg_price,
            AVG(volatility) as avg_volatility
        FROM {view_name}
        WHERE volatility > 0
        GROUP BY symbol
        """
        
        df = self.spark.sql(query)
        
        print("üìã Plan d'ex√©cution PHYSIQUE:")
        print("-" * 60)
        df.explain(mode='formatted')
        
        print("\nüìã Plan d'ex√©cution SIMPLE:")
        print("-" * 60)
        df.explain(mode='simple')
        
        print("\nüìã Plan d'ex√©cution √âTENDU:")
        print("-" * 60)
        df.explain(mode='extended')
        
        print("\nüìã Plan d'ex√©cution AVEC CO√õT:")
        print("-" * 60)
        df.explain(mode='cost')
        
    def run_all_analyses(self, table_name="stock_aggregates"):
        """
        Ex√©cute toutes les analyses SQL
        
        Args:
            table_name: Nom de la table √† analyser
        """
        print("\n" + "="*70)
        print("üöÄ D√âMARRAGE DE L'ANALYSE SPARK SQL COMPL√àTE")
        print("="*70)
        
        # Cr√©er la vue temporaire
        if not self.create_temp_view(table_name, "stock_view"):
            print("‚ùå Impossible de cr√©er la vue. Assurez-vous que le streaming est actif.")
            return
        
        # Attendre un peu pour accumuler des donn√©es
        print("\n‚è≥ Attente de 10 secondes pour accumuler des donn√©es...\n")
        time.sleep(10)
        
        # Ex√©cuter les analyses
        self.query_average_price_by_symbol()
        self.query_top_volatile_stocks()
        self.query_high_volume_periods(volume_threshold=500000)
        
        # Analyser les performances du cache
        self.compare_cache_performance()
        
        # Analyser le plan d'ex√©cution
        self.analyze_execution_plan()
        
        print("\n‚úÖ Analyse Spark SQL termin√©e!")

if __name__ == "__main__":
    # Ce module doit √™tre utilis√© avec un pipeline de streaming actif
    print("‚ö†Ô∏è  Ce module doit √™tre utilis√© avec spark_streaming_pipeline.py")
    print("    Utilisez main.py pour ex√©cuter le pipeline complet.")

