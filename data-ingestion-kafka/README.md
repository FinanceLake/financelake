###                                               FinanceLake â€“ Mini-Projet d'Ingestion de DonnÃ©es 

Objectif

Ce projet a pour but de mettre en place une architecture d'ingestion de donnÃ©es en temps rÃ©el, en simulant des donnÃ©es financiÃ¨res, afin de les stocker dans un Data Lake (Delta Lake) pour une analyse ultÃ©rieure.

 Architecture ProposÃ©e

![Architecture du projet](../resources/img/architecture.png)

 Cette architecture suit une approche moderne orientÃ©e streaming pour assurer une ingestion rapide, fiable et Ã©volutive.

Justification des choix technologiques

Apache Kafka : UtilisÃ© comme broker de messages pour transporter des Ã©vÃ©nements en temps rÃ©el. C'est un systÃ¨me scalable, dÃ©centralisÃ© et hautement performant.

Spark Structured Streaming : Permet de consommer les donnÃ©es de Kafka, de les transformer et de les stocker en continu. Sa compatibilitÃ© native avec Kafka et Delta Lake en fait un excellent choix pour le traitement de flux.

Delta Lake : Fournit une couche de stockage transactionnelle (ACID) sur les fichiers Parquet. Il permet une gestion fiable et versionnÃ©e des donnÃ©es dans un Data Lake.

Producteur Python simulÃ© : Une source de donnÃ©es simple pour simuler un flux continu de valeurs boursiÃ¨res.

Fonctionnement

1. Lancer Kafka (assumÃ© comme dÃ©jÃ  configurÃ©)

DÃ©marrer Zookeeper puis Kafka :
# bin/zookeeper-server-start.sh config/zookeeper.properties
# bin/kafka-server-start.sh config/server.properties
CrÃ©er le topic "finance" :
# bin/kafka-topics.sh --create --topic finance --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

2. ExÃ©cuter le producteur
# cd ingestion-kafka/producer
# python3 producer.py

Cela envoie des messages de la forme :  
> {"symbol": "AAPL", "price": 174.5}

3. Lancer Spark Structured Streaming
# cd ingestion-kafka/spark
# spark-submit kafka_to_delta.py
Les donnÃ©es seront stockÃ©es dans data/finance_delta/ au format Delta Lake.

ğŸš€ Auteur

Nom : Amina ELBAYYADI
Formation : Master Big Data & Intelligence Artificielle